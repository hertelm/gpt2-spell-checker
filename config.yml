model: "gpt2-large"
tokenizer: "gpt2"
n_words: 50000
maximum_edit_distance: 3
minimum_length_per_ed:
  1: 1
  2: 4
  3: 6
beam_width: 10
penalties:
  0: 0
  1: 3
  2: 9
  3: 15
correct_real_words: True
real_word_penalty: 0
first_char_penalty: 3
prune_candidates: True
prune_beams: True
pruning_delta: 5
verbose: True